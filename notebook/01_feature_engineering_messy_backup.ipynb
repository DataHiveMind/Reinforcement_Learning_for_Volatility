{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for infinite or extreme values\n",
    "inf_count = np.isinf(all_features.select_dtypes(include=[np.number])).sum()\n",
    "if inf_count.sum() > 0:\n",
    "    print(\"âš ï¸ Features with infinite values:\")\n",
    "    print(inf_count[inf_count > 0])\n",
    "else:\n",
    "    print(\"âœ… No infinite values detected\")\n",
    "\n",
    "# Check for extreme outliers (beyond 5 std deviations)\n",
    "print(\"\\nðŸ“Š Extreme Value Detection (>5Ïƒ):\")\n",
    "for col in all_features.select_dtypes(include=[np.number]).columns:\n",
    "    mean = all_features[col].mean()\n",
    "    std = all_features[col].std()\n",
    "    outliers = ((all_features[col] - mean).abs() > 5 * std).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"  {col}: {outliers} extreme values ({outliers/len(all_features)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ef458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value analysis\n",
    "missing_pct = (all_features.isnull().sum() / len(all_features) * 100).sort_values(ascending=False)\n",
    "missing_features = missing_pct[missing_pct > 0]\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(\"ðŸ“‰ Features with Missing Values:\")\n",
    "    print(missing_features)\n",
    "\n",
    "    # Visualize missing data pattern\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(\n",
    "        all_features[missing_features.index].isnull(), cbar=True, cmap=\"viridis\", yticklabels=False\n",
    "    )\n",
    "    plt.title(\"Missing Data Pattern\", fontsize=14)\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âœ… No missing values in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acffd3a2",
   "metadata": {},
   "source": [
    "## 9. Save Processed Features\n",
    "\n",
    "Export the engineered features to multiple formats for downstream analysis and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1389b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = project_root / 'data' / 'processed'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save to multiple formats\n",
    "# 1. Parquet (efficient, preserves dtypes)\n",
    "parquet_path = output_dir / f'{SYMBOL}_engineered_features.parquet'\n",
    "all_features.to_parquet(parquet_path, compression='snappy')\n",
    "print(f\"âœ… Saved to Parquet: {parquet_path}\")\n",
    "\n",
    "# 2. CSV (human-readable)\n",
    "csv_path = output_dir / f'{SYMBOL}_engineered_features.csv'\n",
    "all_features.to_csv(csv_path)\n",
    "print(f\"âœ… Saved to CSV: {csv_path}\")\n",
    "\n",
    "# 3. HDF5 (for large datasets)\n",
    "h5_path = output_dir / f'{SYMBOL}_engineered_features.h5'\n",
    "all_features.to_hdf(h5_path, key='features', mode='w', complevel=9)\n",
    "print(f\"âœ… Saved to HDF5: {h5_path}\")\n",
    "\n",
    "# Save feature metadata\n",
    "metadata = {\n",
    "    'symbol': SYMBOL,\n",
    "    'data_source': 'OpenBB Terminal',\n",
    "    'date_range': {\n",
    "        'start': START_DATE,\n",
    "        'end': END_DATE,\n",
    "    },\n",
    "    'n_samples': len(all_features),\n",
    "    'n_features': len(all_features.columns),\n",
    "    'time_range': {\n",
    "        'start': str(all_features.index.min()),\n",
    "        'end': str(all_features.index.max()),\n",
    "    },\n",
    "    'features': {\n",
    "        'microstructure': [col for col in all_features.columns if any(\n",
    "            x in col for x in ['imbalance', 'spread', 'vpin', 'microprice', 'realized_vol']\n",
    "        )],\n",
    "        'options': [col for col in all_features.columns if any(\n",
    "            x in col for x in ['iv_skew', 'term', 'vol_of_vol', 'delta', 'gamma', 'vega']\n",
    "        )],\n",
    "        'market': [col for col in all_features.columns if col in ['price', 'returns', 'log_returns']],\n",
    "    },\n",
    "    'missing_values': missing_pct.to_dict() if 'missing_pct' in locals() else {},\n",
    "    'correlations_high': high_corr_df.to_dict('records') if 'high_corr_df' in locals() and len(high_corr_pairs) > 0 else []\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "metadata_path = output_dir / f'{SYMBOL}_features_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"âœ… Saved metadata: {metadata_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Feature Engineering Complete!\")\n",
    "print(f\"   Symbol: {SYMBOL}\")\n",
    "print(f\"   Total Features: {len(all_features.columns)}\")\n",
    "print(f\"   Total Samples: {len(all_features)}\")\n",
    "print(f\"   Output Directory: {output_dir}\")print(f\"   Output Directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cab0b0",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "**Microstructure Features Engineered:**\n",
    "- Order book imbalance (levels 1, 5, 10)\n",
    "- Microprice (volume-weighted mid)\n",
    "- Spread dynamics (absolute, relative, normalized)\n",
    "- VPIN (trade flow toxicity)\n",
    "- Realized volatility (5, 20, 60 period windows)\n",
    "\n",
    "**Options Features Engineered:**\n",
    "- IV skew (30d, 60d maturities)\n",
    "- Term structure slope and curvature\n",
    "- Vol-of-vol (volatility of implied volatility)\n",
    "- Volatility smile curvature\n",
    "- Greeks exposure (delta, gamma, vega)\n",
    "\n",
    "**Data Source:**\n",
    "- âœ… Real market data from OpenBB Terminal\n",
    "- âœ… Yahoo Finance (free tier)\n",
    "- âœ… Optional: Polygon, Alpha Vantage, CBOE with API keys\n",
    "\n",
    "**Data Quality:**\n",
    "- âœ… No constant features\n",
    "- âœ… No infinite values\n",
    "- âœ… Correlations analyzed\n",
    "- âœ… Temporal stability checked\n",
    "- âœ… Ready for model training\n",
    "\n",
    "**Next Steps:**\n",
    "1. **Notebook 02**: Generate volatility labels and trading targets\n",
    "2. **Notebook 03**: Build microstructure trading signals\n",
    "3. **Notebook 04**: Test RL environment with engineered features\n",
    "4. **Notebook 05**: Train and evaluate RL agents (PPO, DDPG, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Project modules\n",
    "from src.data.loader import DataLoader, MarketDataLoader\n",
    "from src.data.preprocess import DataPreprocessor\n",
    "from src.data.feature_engineering import (\n",
    "    MicrostructureFeatureEngineer,\n",
    "    OptionsFeatureEngineer,\n",
    "    FeatureEngineeringPipeline\n",
    ")\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f29a9",
   "metadata": {},
   "source": [
    "### 1.1 Configure Data Loading\n",
    "\n",
    "Set symbol, date range, and data provider preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic order book data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Time range\n",
    "n_samples = 10000\n",
    "dates = pd.date_range(\"2024-01-01\", periods=n_samples, freq=\"1min\")\n",
    "\n",
    "# Base price with realistic dynamics\n",
    "base_price = 100\n",
    "returns = np.random.normal(0, 0.0002, n_samples)\n",
    "prices = base_price * np.exp(np.cumsum(returns))\n",
    "\n",
    "# Order book levels (10 levels)\n",
    "order_book_data = []\n",
    "\n",
    "for i, (date, price) in enumerate(zip(dates, prices)):\n",
    "    # Spread that varies with volatility\n",
    "    volatility = np.std(returns[max(0, i - 20) : i + 1])\n",
    "    spread = max(0.01, price * volatility * 2)\n",
    "\n",
    "    # Generate 10 levels\n",
    "    for level in range(1, 11):\n",
    "        bid_price = price - spread * level / 2\n",
    "        ask_price = price + spread * level / 2\n",
    "\n",
    "        # Volume decreases with level\n",
    "        base_volume = np.random.exponential(1000)\n",
    "        bid_volume = base_volume * (1.1 - level * 0.05) * np.random.uniform(0.8, 1.2)\n",
    "        ask_volume = base_volume * (0.9 + level * 0.05) * np.random.uniform(0.8, 1.2)\n",
    "\n",
    "        order_book_data.append(\n",
    "            {\n",
    "                \"timestamp\": date,\n",
    "                \"level\": level,\n",
    "                f\"bid_price_{level}\": bid_price,\n",
    "                f\"ask_price_{level}\": ask_price,\n",
    "                f\"bid_size_{level}\": bid_volume,\n",
    "                f\"ask_size_{level}\": ask_volume,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Create DataFrame and pivot\n",
    "ob_df = pd.DataFrame(order_book_data)\n",
    "ob_pivot = ob_df.groupby(\"timestamp\").first().reset_index()\n",
    "\n",
    "# Add aggregate columns\n",
    "ob_pivot[\"mid_price\"] = (ob_pivot[\"bid_price_1\"] + ob_pivot[\"ask_price_1\"]) / 2\n",
    "ob_pivot[\"spread\"] = ob_pivot[\"ask_price_1\"] - ob_pivot[\"bid_price_1\"]\n",
    "ob_pivot[\"total_bid_size\"] = ob_pivot[[f\"bid_size_{i}\" for i in range(1, 11)]].sum(axis=1)\n",
    "ob_pivot[\"total_ask_size\"] = ob_pivot[[f\"ask_size_{i}\" for i in range(1, 11)]].sum(axis=1)\n",
    "\n",
    "print(f\"Generated order book data: {len(ob_pivot)} snapshots\")\n",
    "print(f\"Shape: {ob_pivot.shape}\")\n",
    "ob_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fb06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic trade data\n",
    "trade_data = []\n",
    "\n",
    "for i, (date, price) in enumerate(zip(dates, prices)):\n",
    "    # Random number of trades per minute\n",
    "    n_trades = np.random.poisson(5)\n",
    "\n",
    "    for _ in range(n_trades):\n",
    "        # Trade price around mid\n",
    "        trade_price = price * (1 + np.random.normal(0, 0.0001))\n",
    "        trade_size = np.random.exponential(100)\n",
    "\n",
    "        # Direction based on price movement\n",
    "        if i > 0:\n",
    "            direction = np.sign(prices[i] - prices[i - 1]) if prices[i] != prices[i - 1] else 0\n",
    "        else:\n",
    "            direction = 0\n",
    "\n",
    "        trade_data.append(\n",
    "            {\n",
    "                \"timestamp\": date + pd.Timedelta(seconds=np.random.randint(0, 60)),\n",
    "                \"price\": trade_price,\n",
    "                \"size\": trade_size,\n",
    "                \"direction\": direction,\n",
    "            }\n",
    "        )\n",
    "\n",
    "trades_df = pd.DataFrame(trade_data).sort_values(\"timestamp\")\n",
    "trades_df = (\n",
    "    trades_df.groupby(\"timestamp\")\n",
    "    .agg({\"price\": \"mean\", \"size\": \"sum\", \"direction\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Add derived features\n",
    "trades_df[\"returns\"] = trades_df[\"price\"].pct_change()\n",
    "trades_df[\"log_returns\"] = np.log(trades_df[\"price\"] / trades_df[\"price\"].shift(1))\n",
    "\n",
    "print(f\"Generated trade data: {len(trades_df)} aggregated trades\")\n",
    "trades_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3690b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic options data\n",
    "options_data = []\n",
    "\n",
    "for date, price in zip(dates[::60], prices[::60]):  # Hourly snapshots\n",
    "    # Multiple maturities\n",
    "    for days_to_expiry in [7, 14, 30, 60, 90]:\n",
    "        # Multiple strikes\n",
    "        for moneyness in np.arange(0.9, 1.11, 0.02):\n",
    "            strike = price * moneyness\n",
    "\n",
    "            # Implied volatility with skew\n",
    "            base_iv = 0.20 + 0.10 * np.sin(len(options_data) * 0.01)  # Time-varying\n",
    "            skew = 0.02 * (1.0 - moneyness)  # Negative skew\n",
    "            iv = base_iv + skew\n",
    "\n",
    "            # Term structure\n",
    "            term_premium = 0.01 * np.sqrt(days_to_expiry / 30)\n",
    "            iv += term_premium\n",
    "\n",
    "            # Greeks (simplified)\n",
    "            delta = moneyness if moneyness < 1 else 1 - (moneyness - 1)\n",
    "            gamma = 0.05 / (abs(1 - moneyness) + 0.1)\n",
    "            vega = 0.1 * np.sqrt(days_to_expiry / 365)\n",
    "\n",
    "            options_data.append(\n",
    "                {\n",
    "                    \"timestamp\": date,\n",
    "                    \"strike\": strike,\n",
    "                    \"days_to_expiry\": days_to_expiry,\n",
    "                    \"moneyness\": moneyness,\n",
    "                    \"iv\": iv,\n",
    "                    \"delta\": delta,\n",
    "                    \"gamma\": gamma,\n",
    "                    \"vega\": vega,\n",
    "                    \"underlying_price\": price,\n",
    "                }\n",
    "            )\n",
    "\n",
    "options_df = pd.DataFrame(options_data)\n",
    "\n",
    "print(f\"Generated options data: {len(options_df)} option snapshots\")\n",
    "print(f\"Unique timestamps: {options_df['timestamp'].nunique()}\")\n",
    "print(f\"Strikes per timestamp: {len(options_df) // options_df['timestamp'].nunique()}\")\n",
    "options_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb755d9",
   "metadata": {},
   "source": [
    "### 1.2 Construct Trade Data\n",
    "\n",
    "Generate trade-level data from OHLCV bars with price, volume, and direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f98b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize microstructure feature engineer\n",
    "micro_config = {\n",
    "    \"imbalance\": {\"levels\": [1, 5, 10]},\n",
    "    \"spread\": {\"normalize\": True},\n",
    "    \"vpin\": {\"window\": 50},\n",
    "    \"volatility\": {\"frequencies\": [5, 20, 60]},\n",
    "}\n",
    "\n",
    "micro_engineer = MicrostructureFeatureEngineer(config=micro_config)\n",
    "\n",
    "# Compute order book imbalance\n",
    "imbalance_features = micro_engineer.compute_order_book_imbalance(ob_pivot, levels=[1, 5, 10])\n",
    "\n",
    "print(\"Order Book Imbalance Features:\")\n",
    "print(imbalance_features.describe())\n",
    "imbalance_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute microprice\n",
    "microprice_features = micro_engineer.compute_microprice(ob_pivot)\n",
    "\n",
    "print(\"Microprice Features:\")\n",
    "print(microprice_features.describe())\n",
    "microprice_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9316933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spread dynamics\n",
    "spread_features = micro_engineer.compute_spread_dynamics(ob_pivot)\n",
    "\n",
    "print(\"Spread Dynamics Features:\")\n",
    "print(spread_features.describe())\n",
    "spread_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e5e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VPIN (trade flow toxicity)\n",
    "# Merge trade data with order book\n",
    "merged_data = pd.merge_asof(\n",
    "    trades_df.sort_values(\"timestamp\"),\n",
    "    ob_pivot[[\"timestamp\", \"total_bid_size\", \"total_ask_size\"]].sort_values(\"timestamp\"),\n",
    "    on=\"timestamp\",\n",
    "    direction=\"nearest\",\n",
    ")\n",
    "\n",
    "vpin_features = micro_engineer.compute_vpin(merged_data, window=50)\n",
    "\n",
    "print(\"VPIN (Flow Toxicity) Features:\")\n",
    "print(vpin_features.describe())\n",
    "vpin_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3bb05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute realized volatility at multiple frequencies\n",
    "vol_features = micro_engineer.compute_realized_volatility(\n",
    "    trades_df, frequencies=[5, 20, 60], price_col=\"price\"\n",
    ")\n",
    "\n",
    "print(\"Realized Volatility Features:\")\n",
    "print(vol_features.describe())\n",
    "vol_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all microstructure features\n",
    "micro_features = pd.concat(\n",
    "    [imbalance_features, microprice_features, spread_features, vpin_features, vol_features], axis=1\n",
    ")\n",
    "\n",
    "# Remove duplicate timestamp column if any\n",
    "micro_features = micro_features.loc[:, ~micro_features.columns.duplicated()]\n",
    "\n",
    "print(f\"\\nðŸ“Š Combined Microstructure Features: {micro_features.shape}\")\n",
    "print(f\"Features: {list(micro_features.columns)[:10]}...\")\n",
    "micro_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d853df1",
   "metadata": {},
   "source": [
    "## 3. Compute Options-Implied Features\n",
    "\n",
    "Using the `OptionsFeatureEngineer` class to extract:\n",
    "- IV skew across strikes\n",
    "- Term structure slope and curvature\n",
    "- Vol-of-vol (volatility of implied volatility)\n",
    "- Volatility smile curvature\n",
    "- Greeks exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize options feature engineer\n",
    "options_config = {\n",
    "    \"skew\": {\"maturities\": [30, 60]},\n",
    "    \"term_structure\": {\"atm_range\": 0.05},\n",
    "    \"vol_of_vol\": {\"window\": 20},\n",
    "    \"greeks\": {\"enabled\": True},\n",
    "}\n",
    "\n",
    "options_engineer = OptionsFeatureEngineer(config=options_config)\n",
    "\n",
    "# Compute IV skew\n",
    "skew_features = options_engineer.compute_iv_skew(options_df, maturities=[30, 60])\n",
    "\n",
    "print(\"IV Skew Features:\")\n",
    "print(skew_features.describe())\n",
    "skew_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1896fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute term structure\n",
    "term_features = options_engineer.compute_term_structure(options_df, atm_range=0.05)\n",
    "\n",
    "print(\"Term Structure Features:\")\n",
    "print(term_features.describe())\n",
    "term_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute vol-of-vol\n",
    "vov_features = options_engineer.compute_vol_of_vol(options_df, maturities=[30, 60], window=20)\n",
    "\n",
    "print(\"Vol-of-Vol Features:\")\n",
    "print(vov_features.describe())\n",
    "vov_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Greeks exposure\n",
    "greeks_features = options_engineer.compute_greeks_exposure(options_df, maturity=30)\n",
    "\n",
    "print(\"Greeks Exposure Features:\")\n",
    "print(greeks_features.describe())\n",
    "greeks_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all options features\n",
    "options_features = pd.concat([skew_features, term_features, vov_features, greeks_features], axis=1)\n",
    "\n",
    "# Remove duplicates\n",
    "options_features = options_features.loc[:, ~options_features.columns.duplicated()]\n",
    "\n",
    "print(f\"\\nðŸ“Š Combined Options Features: {options_features.shape}\")\n",
    "print(f\"Features: {list(options_features.columns)}\")\n",
    "options_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c2aa07",
   "metadata": {},
   "source": [
    "## 4. Merge All Features\n",
    "\n",
    "Combine microstructure and options features into a single dataset aligned by timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc802d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge microstructure and options features\n",
    "# Options data is at lower frequency, so merge_asof with nearest\n",
    "all_features = pd.merge_asof(\n",
    "    micro_features.sort_values(\"timestamp\"),\n",
    "    options_features.sort_values(\"timestamp\"),\n",
    "    on=\"timestamp\",\n",
    "    direction=\"nearest\",\n",
    "    tolerance=pd.Timedelta(\"1H\"),  # Max 1 hour gap\n",
    ")\n",
    "\n",
    "# Add price and returns from trades\n",
    "all_features = pd.merge_asof(\n",
    "    all_features.sort_values(\"timestamp\"),\n",
    "    trades_df[[\"timestamp\", \"price\", \"returns\", \"log_returns\"]].sort_values(\"timestamp\"),\n",
    "    on=\"timestamp\",\n",
    "    direction=\"nearest\",\n",
    ")\n",
    "\n",
    "# Set timestamp as index\n",
    "all_features = all_features.set_index(\"timestamp\")\n",
    "\n",
    "# Drop rows with too many NaNs\n",
    "all_features = all_features.dropna(thresh=len(all_features.columns) * 0.7)\n",
    "\n",
    "print(f\"\\nâœ… Final Feature Dataset: {all_features.shape}\")\n",
    "print(f\"Time range: {all_features.index.min()} to {all_features.index.max()}\")\n",
    "print(f\"Features: {len(all_features.columns)}\")\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(all_features.isnull().sum()[all_features.isnull().sum() > 0])\n",
    "\n",
    "all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0894aa02",
   "metadata": {},
   "source": [
    "## 5. Feature Distributions & Statistics\n",
    "\n",
    "Analyze the statistical properties and distributions of engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"ðŸ“Š Feature Summary Statistics:\\n\")\n",
    "print(all_features.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of key microstructure features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "fig.suptitle(\"Microstructure Feature Distributions\", fontsize=16, y=1.02)\n",
    "\n",
    "micro_cols = [\n",
    "    \"imbalance_level_1\",\n",
    "    \"imbalance_level_5\",\n",
    "    \"vpin\",\n",
    "    \"relative_spread\",\n",
    "    \"realized_vol_20\",\n",
    "    \"microprice\",\n",
    "]\n",
    "\n",
    "for idx, col in enumerate(micro_cols):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    if col in all_features.columns:\n",
    "        all_features[col].hist(bins=50, ax=ax, edgecolor=\"black\", alpha=0.7)\n",
    "        ax.set_title(col)\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85802232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of key options features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle(\"Options Feature Distributions\", fontsize=16, y=1.02)\n",
    "\n",
    "options_cols = [\"iv_skew_30d\", \"term_slope\", \"vol_of_vol_30d\", \"delta_exposure\"]\n",
    "\n",
    "for idx, col in enumerate(options_cols):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    if col in all_features.columns:\n",
    "        all_features[col].dropna().hist(bins=50, ax=ax, edgecolor=\"black\", alpha=0.7, color=\"coral\")\n",
    "        ax.set_title(col)\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3af4d0",
   "metadata": {},
   "source": [
    "## 6. Feature Correlation Analysis\n",
    "\n",
    "Identify multicollinearity and relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2631aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = all_features.corr()\n",
    "\n",
    "# Plot full correlation heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    cmap=\"RdBu_r\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"label\": \"Correlation\"},\n",
    "    annot=False,\n",
    ")\n",
    "plt.title(\"Feature Correlation Heatmap\", fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed73315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find highly correlated features (potential redundancy)\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append(\n",
    "                {\n",
    "                    \"Feature 1\": correlation_matrix.columns[i],\n",
    "                    \"Feature 2\": correlation_matrix.columns[j],\n",
    "                    \"Correlation\": correlation_matrix.iloc[i, j],\n",
    "                }\n",
    "            )\n",
    "\n",
    "if high_corr_pairs:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values(\n",
    "        \"Correlation\", key=abs, ascending=False\n",
    "    )\n",
    "    print(\"âš ï¸ Highly Correlated Feature Pairs (|r| > 0.8):\\n\")\n",
    "    print(high_corr_df)\n",
    "else:\n",
    "    print(\"âœ… No highly correlated feature pairs found (|r| > 0.8)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29313175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive correlation plot with plotly\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=correlation_matrix.values,\n",
    "        x=correlation_matrix.columns,\n",
    "        y=correlation_matrix.columns,\n",
    "        colorscale=\"RdBu_r\",\n",
    "        zmid=0,\n",
    "        zmin=-1,\n",
    "        zmax=1,\n",
    "        text=correlation_matrix.values,\n",
    "        hovertemplate=\"%{x}<br>%{y}<br>Correlation: %{z:.3f}<extra></extra>\",\n",
    "        colorbar=dict(title=\"Correlation\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Interactive Feature Correlation Matrix\",\n",
    "    width=900,\n",
    "    height=800,\n",
    "    xaxis={\"side\": \"bottom\"},\n",
    "    yaxis={\"autorange\": \"reversed\"},\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e9e97",
   "metadata": {},
   "source": [
    "### 1.3 Load Options Data\n",
    "\n",
    "Fetch real options chains with implied volatilities and Greeks from OpenBB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series of key features\n",
    "fig = make_subplots(\n",
    "    rows=3,\n",
    "    cols=2,\n",
    "    subplot_titles=[\n",
    "        \"Price\",\n",
    "        \"Order Book Imbalance\",\n",
    "        \"VPIN (Flow Toxicity)\",\n",
    "        \"IV Skew\",\n",
    "        \"Realized Volatility\",\n",
    "        \"Term Structure\",\n",
    "    ],\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.1,\n",
    ")\n",
    "\n",
    "# Price\n",
    "if \"price\" in all_features.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=all_features.index, y=all_features[\"price\"], name=\"Price\", line=dict(color=\"blue\")\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "# Imbalance\n",
    "if \"imbalance_level_1\" in all_features.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=all_features.index,\n",
    "            y=all_features[\"imbalance_level_1\"],\n",
    "            name=\"Imbalance L1\",\n",
    "            line=dict(color=\"green\"),\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "# VPIN\n",
    "if \"vpin\" in all_features.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=all_features.index, y=all_features[\"vpin\"], name=\"VPIN\", line=dict(color=\"red\")\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "# IV Skew\n",
    "if \"iv_skew_30d\" in all_features.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=all_features.index,\n",
    "            y=all_features[\"iv_skew_30d\"],\n",
    "            name=\"IV Skew 30d\",\n",
    "            line=dict(color=\"purple\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "# Realized Vol\n",
    "if \"realized_vol_20\" in all_features.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=all_features.index,\n",
    "            y=all_features[\"realized_vol_20\"],\n",
    "            name=\"Realized Vol 20\",\n",
    "            line=dict(color=\"orange\"),\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "# Term Structure\n",
    "if \"term_slope\" in all_features.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=all_features.index,\n",
    "            y=all_features[\"term_slope\"],\n",
    "            name=\"Term Slope\",\n",
    "            line=dict(color=\"teal\"),\n",
    "        ),\n",
    "        row=3,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "fig.update_layout(title_text=\"Feature Time Series\", height=900, showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling statistics to check stability\n",
    "window = 500  # ~8 hours of minute data\n",
    "\n",
    "rolling_stats = pd.DataFrame(index=all_features.index)\n",
    "\n",
    "# Select key features for stability analysis\n",
    "key_features = [\"imbalance_level_1\", \"vpin\", \"realized_vol_20\", \"iv_skew_30d\"]\n",
    "\n",
    "for feature in key_features:\n",
    "    if feature in all_features.columns:\n",
    "        rolling_stats[f\"{feature}_mean\"] = all_features[feature].rolling(window).mean()\n",
    "        rolling_stats[f\"{feature}_std\"] = all_features[feature].rolling(window).std()\n",
    "\n",
    "# Plot rolling mean and std\n",
    "fig, axes = plt.subplots(len(key_features), 1, figsize=(14, 12))\n",
    "fig.suptitle(f\"Rolling Statistics (Window={window})\", fontsize=16, y=0.995)\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    if feature in all_features.columns:\n",
    "        ax = axes[idx]\n",
    "\n",
    "        # Plot rolling mean\n",
    "        ax.plot(\n",
    "            rolling_stats.index,\n",
    "            rolling_stats[f\"{feature}_mean\"],\n",
    "            label=\"Mean\",\n",
    "            color=\"blue\",\n",
    "            linewidth=1.5,\n",
    "        )\n",
    "\n",
    "        # Plot rolling std as shaded area\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.fill_between(\n",
    "            rolling_stats.index,\n",
    "            0,\n",
    "            rolling_stats[f\"{feature}_std\"],\n",
    "            alpha=0.3,\n",
    "            color=\"red\",\n",
    "            label=\"Std Dev\",\n",
    "        )\n",
    "\n",
    "        ax.set_title(feature, fontsize=12)\n",
    "        ax.set_ylabel(\"Mean\", color=\"blue\")\n",
    "        ax2.set_ylabel(\"Std Dev\", color=\"red\")\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "        if idx == len(key_features) - 1:\n",
    "            ax.set_xlabel(\"Time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d18bb3",
   "metadata": {},
   "source": [
    "## 8. Data Quality Validation\n",
    "\n",
    "Check for constant features, extreme values, missing data, and other quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f8525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for constant or near-constant features\n",
    "variance_check = all_features.var()\n",
    "low_variance = variance_check[variance_check < 1e-10]\n",
    "\n",
    "if len(low_variance) > 0:\n",
    "    print(\"âš ï¸ Low Variance Features (potential constants):\")\n",
    "    print(low_variance)\n",
    "else:\n",
    "    print(\"âœ… All features have sufficient variance\")\n",
    "\n",
    "print(f\"\\nVariance Statistics:\")\n",
    "print(variance_check.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real options data using OpenBB\n",
    "print(f\"ðŸŽ¯ Loading options data for {SYMBOL}...\")\n",
    "\n",
    "try:\n",
    "    # Load options chain\n",
    "    options_df = loader.load_openbb_options(\n",
    "        symbol=SYMBOL,\n",
    "        provider=\"cboe\",  # or \"tradier\", \"intrinio\" with API keys\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Loaded {len(options_df)} options contracts from OpenBB\")\n",
    "    print(f\"   Columns: {list(options_df.columns)[:10]}...\")\n",
    "\n",
    "    # Process options data\n",
    "    if \"expiration\" in options_df.columns:\n",
    "        options_df[\"days_to_expiry\"] = (\n",
    "            pd.to_datetime(options_df[\"expiration\"]) - pd.Timestamp.now()\n",
    "        ).dt.days\n",
    "\n",
    "    # Calculate moneyness\n",
    "    current_price = (\n",
    "        trades_df[\"price\"].iloc[-1] if len(trades_df) > 0 else prices_df[\"close\"].iloc[-1]\n",
    "    )\n",
    "\n",
    "    if \"strike\" in options_df.columns:\n",
    "        options_df[\"moneyness\"] = options_df[\"strike\"] / current_price\n",
    "\n",
    "    # Add timestamp\n",
    "    if \"timestamp\" not in options_df.columns:\n",
    "        options_df[\"timestamp\"] = pd.Timestamp.now()\n",
    "\n",
    "    # Rename IV column if needed\n",
    "    iv_col = next(\n",
    "        (col for col in options_df.columns if \"iv\" in col.lower() or \"implied\" in col.lower()), None\n",
    "    )\n",
    "    if iv_col and iv_col != \"iv\":\n",
    "        options_df[\"iv\"] = options_df[iv_col]\n",
    "\n",
    "    print(\n",
    "        f\"   Expirations available: {options_df['expiration'].nunique() if 'expiration' in options_df.columns else 'N/A'}\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not load real options data: {e}\")\n",
    "    print(\"   Generating synthetic options data as fallback...\")\n",
    "\n",
    "    # Fallback: Generate synthetic options data\n",
    "    options_data = []\n",
    "    current_price = trades_df[\"price\"].iloc[-1] if len(trades_df) > 0 else 100\n",
    "\n",
    "    # Sample timestamps (daily snapshots)\n",
    "    sample_timestamps = pd.date_range(\n",
    "        start=prices_df[\"timestamp\"].min(), end=prices_df[\"timestamp\"].max(), freq=\"1D\"\n",
    "    )\n",
    "\n",
    "    for timestamp in sample_timestamps:\n",
    "        # Get price at this timestamp\n",
    "        price_row = (\n",
    "            prices_df[prices_df[\"timestamp\"] <= timestamp].iloc[-1] if len(prices_df) > 0 else None\n",
    "        )\n",
    "        price = (\n",
    "            price_row[\"close\"] if price_row is not None and \"close\" in price_row else current_price\n",
    "        )\n",
    "\n",
    "        # Multiple maturities\n",
    "        for days_to_expiry in [7, 14, 30, 60, 90]:\n",
    "            # Multiple strikes\n",
    "            for moneyness in np.arange(0.9, 1.11, 0.02):\n",
    "                strike = price * moneyness\n",
    "\n",
    "                # Implied volatility with skew\n",
    "                base_iv = 0.20  # 20% base IV\n",
    "                skew = 0.02 * (1.0 - moneyness)  # Negative skew\n",
    "                iv = base_iv + skew\n",
    "\n",
    "                # Term structure\n",
    "                term_premium = 0.01 * np.sqrt(days_to_expiry / 30)\n",
    "                iv += term_premium\n",
    "\n",
    "                # Greeks (simplified)\n",
    "                delta = moneyness if moneyness < 1 else 1 - (moneyness - 1)\n",
    "                gamma = 0.05 / (abs(1 - moneyness) + 0.1)\n",
    "                vega = 0.1 * np.sqrt(days_to_expiry / 365)\n",
    "\n",
    "                options_data.append(\n",
    "                    {\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"strike\": strike,\n",
    "                        \"days_to_expiry\": days_to_expiry,\n",
    "                        \"moneyness\": moneyness,\n",
    "                        \"iv\": iv,\n",
    "                        \"delta\": delta,\n",
    "                        \"gamma\": gamma,\n",
    "                        \"vega\": vega,\n",
    "                        \"underlying_price\": price,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    options_df = pd.DataFrame(options_data)\n",
    "    print(f\"âœ… Generated {len(options_df)} synthetic options contracts\")\n",
    "\n",
    "print(f\"\\nOptions data summary:\")\n",
    "print(f\"   Unique timestamps: {options_df['timestamp'].nunique()}\")\n",
    "if \"strike\" in options_df.columns and \"timestamp\" in options_df.columns:\n",
    "    print(f\"   Strikes per timestamp: {len(options_df) // options_df['timestamp'].nunique()}\")\n",
    "\n",
    "options_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11521cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trade-level data from OHLCV bars\n",
    "print(f\"ðŸ“ˆ Constructing trade data from price bars...\")\n",
    "\n",
    "trade_data = []\n",
    "\n",
    "for idx, row in prices_df.iterrows():\n",
    "    timestamp = row[\"timestamp\"]\n",
    "    close = row[\"close\"] if \"close\" in row else row[\"Close\"]\n",
    "    volume = row[\"volume\"] if \"volume\" in row else row[\"Volume\"]\n",
    "\n",
    "    # Estimate number of trades per bar (varies by liquidity)\n",
    "    n_trades = max(1, int(volume / 100))  # Rough approximation\n",
    "    n_trades = min(n_trades, 50)  # Cap to avoid memory issues\n",
    "\n",
    "    # Generate individual trades within the bar\n",
    "    for i in range(n_trades):\n",
    "        # Price variation within the bar\n",
    "        trade_price = close * (1 + np.random.normal(0, 0.0001))\n",
    "        trade_size = volume / n_trades * np.random.uniform(0.5, 1.5)\n",
    "\n",
    "        # Add microsecond offset within the bar\n",
    "        trade_time = timestamp + pd.Timedelta(seconds=(60 * i / n_trades))\n",
    "\n",
    "        trade_data.append(\n",
    "            {\n",
    "                \"timestamp\": trade_time,\n",
    "                \"price\": trade_price,\n",
    "                \"size\": trade_size,\n",
    "            }\n",
    "        )\n",
    "\n",
    "trades_df = pd.DataFrame(trade_data).sort_values(\"timestamp\")\n",
    "\n",
    "# Aggregate to reduce data size if needed\n",
    "trades_df = (\n",
    "    trades_df.groupby(pd.Grouper(key=\"timestamp\", freq=\"1min\"))\n",
    "    .agg(\n",
    "        {\n",
    "            \"price\": \"mean\",\n",
    "            \"size\": \"sum\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Compute trade direction using tick rule\n",
    "trades_df[\"direction\"] = np.sign(trades_df[\"price\"].diff())\n",
    "trades_df.loc[trades_df[\"direction\"] == 0, \"direction\"] = np.nan\n",
    "trades_df[\"direction\"] = trades_df[\"direction\"].ffill().fillna(0)\n",
    "\n",
    "# Add returns\n",
    "trades_df[\"returns\"] = trades_df[\"price\"].pct_change()\n",
    "trades_df[\"log_returns\"] = np.log(trades_df[\"price\"] / trades_df[\"price\"].shift(1))\n",
    "\n",
    "print(f\"âœ… Constructed trade data: {len(trades_df)} aggregated trades\")\n",
    "print(f\"   Average trade size: {trades_df['size'].mean():.2f}\")\n",
    "print(f\"   Price range: ${trades_df['price'].min():.2f} - ${trades_df['price'].max():.2f}\")\n",
    "\n",
    "trades_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1fb19",
   "metadata": {},
   "source": [
    "## 1. Load Real Market Data via OpenBB Terminal\n",
    "\n",
    "Load historical price data, construct order book, and prepare trade-level data.\n",
    "\n",
    "**Data Provider**: OpenBB Terminal (Yahoo Finance, Polygon, Alpha Vantage, etc.)  \n",
    "**Symbol**: Configurable (default: SPY)  \n",
    "**Frequency**: 1-minute bars for microstructure analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbe661",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0. Setup & Imports\n",
    "\n",
    "Configure the environment and import required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a923d",
   "metadata": {},
   "source": [
    "# Feature Engineering for Volatility Trading with OpenBB\n",
    "\n",
    "This notebook loads **real market data** using OpenBB Terminal and engineers features for RL trading models.\n",
    "\n",
    "## Data Sources\n",
    "- **Historical Prices**: OHLCV data from Yahoo Finance, Polygon, or Alpha Vantage\n",
    "- **Order Book**: Approximated from OHLCV (or real Level 2 with Polygon API)\n",
    "- **Options Data**: Real options chains with IVs and Greeks from CBOE/Tradier\n",
    "\n",
    "## Features Engineered\n",
    "\n",
    "**Microstructure Features:**\n",
    "- Order book imbalance (levels 1, 5, 10)\n",
    "- Microprice (volume-weighted mid)\n",
    "- Spread dynamics (absolute, relative, normalized)\n",
    "- VPIN (Volume-Synchronized Probability of Informed Trading)\n",
    "- Realized volatility (5, 20, 60 period windows)\n",
    "\n",
    "**Options-Implied Features:**\n",
    "- IV skew across strikes (30d, 60d maturities)\n",
    "- Term structure slope and curvature\n",
    "- Vol-of-vol (volatility of implied volatility)\n",
    "- Volatility smile curvature\n",
    "- Greeks exposure (delta, gamma, vega, theta, rho)\n",
    "\n",
    "## Output\n",
    "- Processed feature dataset ready for RL model training\n",
    "- Feature distributions and correlation analysis\n",
    "- Temporal stability checks\n",
    "- Data quality validation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
