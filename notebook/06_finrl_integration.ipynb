{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22126bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.models.finrl_agent import VolatilityFinRLAgent, FinRLEnsembleAgent\n",
    "from src.envs.volatility_env import VolatilityTradingEnv\n",
    "from src.utils.logging_utils import get_logger\n",
    "\n",
    "# Setup\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "%matplotlib inline\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76918c",
   "metadata": {},
   "source": [
    "## 1. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d4397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for demonstration\n",
    "# In production, replace with actual market data\n",
    "\n",
    "n_days = 1000\n",
    "dates = pd.date_range(start=\"2020-01-01\", periods=n_days, freq=\"D\")\n",
    "\n",
    "np.random.seed(42)\n",
    "price = 100 * np.exp(np.cumsum(np.random.randn(n_days) * 0.02))\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": dates,\n",
    "        \"close\": price,\n",
    "        \"high\": price * (1 + np.abs(np.random.randn(n_days) * 0.01)),\n",
    "        \"low\": price * (1 - np.abs(np.random.randn(n_days) * 0.01)),\n",
    "        \"volume\": np.random.randint(1000000, 10000000, n_days),\n",
    "        \"volatility\": np.abs(np.random.randn(n_days) * 0.2 + 0.15),\n",
    "        \"bid_ask_spread\": np.abs(np.random.randn(n_days) * 0.002 + 0.001),\n",
    "        \"order_imbalance\": np.random.randn(n_days) * 0.1,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e59f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Price\n",
    "axes[0].plot(data[\"date\"], data[\"close\"], label=\"Close Price\")\n",
    "axes[0].fill_between(data[\"date\"], data[\"low\"], data[\"high\"], alpha=0.3)\n",
    "axes[0].set_title(\"Price History\")\n",
    "axes[0].set_ylabel(\"Price\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Volatility\n",
    "axes[1].plot(data[\"date\"], data[\"volatility\"], color=\"orange\", label=\"Volatility\")\n",
    "axes[1].set_title(\"Volatility Over Time\")\n",
    "axes[1].set_ylabel(\"Volatility\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Microstructure signals\n",
    "axes[2].plot(data[\"date\"], data[\"bid_ask_spread\"], label=\"Bid-Ask Spread\", alpha=0.7)\n",
    "axes[2].plot(data[\"date\"], data[\"order_imbalance\"], label=\"Order Imbalance\", alpha=0.7)\n",
    "axes[2].set_title(\"Microstructure Signals\")\n",
    "axes[2].set_ylabel(\"Value\")\n",
    "axes[2].set_xlabel(\"Date\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d7b06f",
   "metadata": {},
   "source": [
    "## 2. Create Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44856891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "# Note: Adjust parameters based on your actual environment implementation\n",
    "\n",
    "env = VolatilityTradingEnv(\n",
    "    data=data,\n",
    "    initial_balance=100000,\n",
    "    # Add other environment-specific parameters\n",
    ")\n",
    "\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a17b466",
   "metadata": {},
   "source": [
    "## 3. Train Single FinRL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33389221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PPO agent\n",
    "ppo_agent = VolatilityFinRLAgent(\n",
    "    env=env,\n",
    "    model_name=\"ppo\",\n",
    "    model_kwargs={\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"n_steps\": 2048,\n",
    "        \"batch_size\": 64,\n",
    "        \"gamma\": 0.99,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Training PPO agent...\")\n",
    "ppo_agent.train(total_timesteps=50000, tb_log_name=\"finrl_ppo_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10514d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PPO agent\n",
    "def evaluate_agent(agent, env, n_episodes=5):\n",
    "    \"\"\"Evaluate agent performance\"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "    return np.mean(episode_rewards), np.std(episode_rewards)\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_agent(ppo_agent, env)\n",
    "print(f\"PPO Performance: {mean_reward:.2f} ± {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b2b90",
   "metadata": {},
   "source": [
    "## 4. Train Ensemble of Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train ensemble\n",
    "ensemble = FinRLEnsembleAgent(\n",
    "    env=env,\n",
    "    model_names=[\"ppo\", \"a2c\", \"sac\"],\n",
    ")\n",
    "\n",
    "print(\"Training ensemble (this may take a while)...\")\n",
    "ensemble.train_all(total_timesteps=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce871e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ensemble with single agents\n",
    "results = {}\n",
    "\n",
    "# Evaluate each agent in ensemble\n",
    "for name, agent in ensemble.agents.items():\n",
    "    mean, std = evaluate_agent(agent, env, n_episodes=5)\n",
    "    results[name] = {\"mean\": mean, \"std\": std}\n",
    "    print(f\"{name.upper()}: {mean:.2f} ± {std:.2f}\")\n",
    "\n",
    "# Plot comparison\n",
    "models = list(results.keys())\n",
    "means = [results[m][\"mean\"] for m in models]\n",
    "stds = [results[m][\"std\"] for m in models]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, means, yerr=stds, capsize=5, alpha=0.7)\n",
    "plt.title(\"FinRL Agents Performance Comparison\")\n",
    "plt.xlabel(\"Agent\")\n",
    "plt.ylabel(\"Mean Episode Reward\")\n",
    "plt.grid(True, axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0467f",
   "metadata": {},
   "source": [
    "## 5. Save and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ea551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save single agent\n",
    "ppo_agent.save(\"../models/finrl_ppo_demo\")\n",
    "print(\"PPO agent saved\")\n",
    "\n",
    "# Save ensemble\n",
    "ensemble.save_all(\"../models/finrl_ensemble_demo\")\n",
    "print(\"Ensemble saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agent for inference\n",
    "loaded_agent = VolatilityFinRLAgent(\n",
    "    env=env,\n",
    "    model_name=\"ppo\",\n",
    ")\n",
    "loaded_agent.load(\"../models/finrl_ppo_demo\")\n",
    "\n",
    "# Test loaded agent\n",
    "mean_reward, std_reward = evaluate_agent(loaded_agent, env, n_episodes=3)\n",
    "print(f\"Loaded agent performance: {mean_reward:.2f} ± {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24ac2b",
   "metadata": {},
   "source": [
    "## 6. Advanced: Hyperparameter Tuning with Optuna\n",
    "\n",
    "FinRL works well with hyperparameter optimization libraries like Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function for hyperparameter tuning\"\"\"\n",
    "\n",
    "    # Sample hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256])\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [512, 1024, 2048])\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.9, 0.9999)\n",
    "\n",
    "    # Create agent with sampled params\n",
    "    agent = VolatilityFinRLAgent(\n",
    "        env=env,\n",
    "        model_name=\"ppo\",\n",
    "        model_kwargs={\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"n_steps\": n_steps,\n",
    "            \"gamma\": gamma,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    agent.train(total_timesteps=20000)\n",
    "\n",
    "    # Evaluate\n",
    "    mean_reward, _ = evaluate_agent(agent, env, n_episodes=3)\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "\n",
    "# Run optimization (uncomment to execute)\n",
    "# study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(), pruner=MedianPruner())\n",
    "# study.optimize(objective, n_trials=20)\n",
    "\n",
    "# print(\"Best hyperparameters:\", study.best_params)\n",
    "# print(\"Best score:\", study.best_value)# print(\"Best score:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3410d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. ✅ Integrated FinRL with volatility trading environment\n",
    "2. ✅ Trained multiple RL algorithms (PPO, A2C, SAC)\n",
    "3. ✅ Created an ensemble of agents\n",
    "4. ✅ Saved and loaded models\n",
    "5. ✅ Demonstrated hyperparameter tuning setup\n",
    "\n",
    "## Next Steps\n",
    "- Use real market data instead of synthetic data\n",
    "- Implement proper train/validation/test split\n",
    "- Add more sophisticated evaluation metrics\n",
    "- Integrate with backtesting framework\n",
    "- Deploy best model for live trading"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
